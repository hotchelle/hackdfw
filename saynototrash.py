# -*- coding: utf-8 -*-
"""SayNoToTrash.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GHi1_-1hdqZDZLdX0L72NoItrH3T24MP
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from PIL import Image
from matplotlib import pyplot as plt
from tensorflow.python.util import compat
from tensorflow.core.protobuf import saved_model_pb2
from google.protobuf import text_format
import pprint
import json
import os

# needed to install object_detection library and enlarge labels
! rm -rf ./models && git clone https://github.com/tensorflow/models.git \
    && sed -i "s#ImageFont.truetype('arial.ttf', 24)#ImageFont.truetype('arial.ttf', 50)#g" ./models/research/object_detection/utils/visualization_utils.py \
    && cp /content/DejaVuSans.ttf /content/arial.ttf

# install object_detection library
! cd models/research \
    && protoc object_detection/protos/*.proto --python_out=. \
    && cp object_detection/packages/tf2/setup.py . && \
    python3 -m pip install --use-feature=2020-resolver .

from object_detection.utils import visualization_utils as vis_util
from object_detection.utils import dataset_util, label_map_util
from object_detection.protos import string_int_label_map_pb2

# reconstruct frozen graph
def reconstruct(pb_path):
    if not os.path.isfile(pb_path):
        print("Error: %s not found" % pb_path)

    print("Reconstructing Tensorflow model")
    detection_graph = tf.Graph()
    with detection_graph.as_default():
        od_graph_def = tf.compat.v1.GraphDef()
        with tf.io.gfile.GFile(pb_path, 'rb') as fid:
            serialized_graph = fid.read()
            od_graph_def.ParseFromString(serialized_graph)
            tf.import_graph_def(od_graph_def, name='')
    print("Success!")
    return detection_graph

# Commented out IPython magic to ensure Python compatibility.
# visualize detection
def image2np(image):
    (w, h) = image.size
    return np.array(image.getdata()).reshape((h, w, 3)).astype(np.uint8)

def image2tensor(image):
    npim = image2np(image)
    return np.expand_dims(npim, axis=0)

# %matplotlib inline
def detect(detection_graph, test_image_path):
    with detection_graph.as_default():
        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.01)
        with tf.compat.v1.Session(graph=detection_graph,config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)) as sess:
            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
            num_detections = detection_graph.get_tensor_by_name('num_detections:0')

            image = Image.open(test_image_path)
            (boxes, scores, classes, num) = sess.run(
                [detection_boxes, detection_scores, detection_classes, num_detections],
                feed_dict={image_tensor: image2tensor(image)}
            )

            npim = image2np(image)
            vis_util.visualize_boxes_and_labels_on_image_array(
                npim,
                np.squeeze(boxes),
                np.squeeze(classes).astype(np.int32),
                np.squeeze(scores),
                category_index,
                use_normalized_coordinates=True,
                line_thickness=15)
            plt.figure(figsize=(12, 8))
            plt.imshow(npim)
            plt.show()
            return scores

DATA_DIR = '/content/'
ANNOTATIONS_FILE = os.path.join(DATA_DIR, 'annotations.json')
NCLASSES = 60

with open(ANNOTATIONS_FILE) as json_file:
    data = json.load(json_file)
    
categories = data['categories']

print('Building label map from examples')

labelmap = string_int_label_map_pb2.StringIntLabelMap()
for idx,category in enumerate(categories):
    item = labelmap.item.add()
    # label map id 0 is reserved for the background label
    item.id = int(category['id'])+1
    item.name = category['name']

with open('./labelmap.pbtxt', 'w') as f:
    f.write(text_format.MessageToString(labelmap))

print('Label map witten to labelmap.pbtxt')

with open('./labelmap.pbtxt') as f:
    pprint.pprint(f.readlines())

label_map = label_map_util.load_labelmap('labelmap.pbtxt')

categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NCLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

detection_graph = reconstruct("/content/ssd_mobilenet_v2_taco_2018_03_29.pb")

score = detect(detection_graph, '/content/gsv_0.jpg')

score = detect(detection_graph, '/content/gsv_1.jpg')

score = detect(detection_graph, '/content/gsv_2.jpg')

score = detect(detection_graph, '/content/gsv_3.jpg')

score = detect(detection_graph, '/content/gsv_4.jpg')

score = detect(detection_graph, '/content/gsv_5.jpg')

score = detect(detection_graph, '/content/gsv_6.jpg')

score = detect(detection_graph, '/content/gsv_7.jpg')

export_var = 0
if score[0][0] > 0.6000:
  export_var += 1

export_var

print(coords)
a = np.array(coords)
s = np.sort(a, axis=0)
l = s.tolist()
flag = True
# API calls optimization
while flag:
    for x in range(len(l) - 1):
        d0 = math.isclose(l[x][0], l[x + 1][0], abs_tol=0.0003)
        d1 = math.isclose(l[x][1], l[x + 1][1], abs_tol=0.0003)
        if d0 and d1:
        # distance = math.sqrt((l[x][0]-l[x+1][0])**2 + (l[x][1]-l[x+1][1])**2)
        # if distance < 0.00025:
            l.pop(x + 1)
            break
        if x >=  len(l) - 2:
            flag = False

import json
my_json_string = """{

   [
      {
         "name":"loc1",
         "prob": "",
         "lon": "",
         "lat": ""
      },

      {
         "name":"loc2",
         "prob": "",
         "lon": "",
         "lat": ""
      },

      {
         "name":"loc3",
         "prob": "",
         "lon": "",
         "lat": ""
      }
      ,

      {
         "name":"loc4",
         "prob": "",
         "lon": "",
         "lat": ""
      }
   ],

   "blog":[
   {
       "name": "Datacamp",
       "URL":"datacamp.com"
   }
  ]
"""
to_python = json.loads(my_json_string)